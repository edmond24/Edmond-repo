{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-1.6.0.tar.gz (2.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.6MB 189kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from spacy)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy)\n",
      "  Downloading murmurhash-0.26.4-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting preshed<0.47.0,>=0.46.0 (from spacy)\n",
      "  Downloading preshed-0.46.4-cp27-cp27m-macosx_10_6_intel.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.4MB/s \n",
      "\u001b[?25hCollecting thinc<6.3.0,>=6.2.0 (from spacy)\n",
      "  Downloading thinc-6.2.0.tar.gz (743kB)\n",
      "\u001b[K    100% |████████████████████████████████| 747kB 636kB/s \n",
      "\u001b[?25hCollecting plac (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cloudpickle in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from spacy)\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.1MB/s \n",
      "\u001b[?25hCollecting sputnik<0.10.0,>=0.9.2 (from spacy)\n",
      "  Downloading sputnik-0.9.3-py2.py3-none-any.whl\n",
      "Collecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 1.5MB/s \n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.10.0 (from thinc<6.3.0,>=6.2.0->spacy)\n",
      "  Downloading tqdm-4.11.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: cytoolz<0.9,>=0.8 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from thinc<6.3.0,>=6.2.0->spacy)\n",
      "Collecting flexmock (from thinc<6.3.0,>=6.2.0->spacy)\n",
      "  Downloading flexmock-0.10.2.tar.gz (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.8MB/s \n",
      "\u001b[?25hCollecting semver (from sputnik<0.10.0,>=0.9.2->spacy)\n",
      "  Downloading semver-2.7.4.tar.gz\n",
      "Building wheels for collected packages: spacy, thinc, pathlib, ujson, flexmock, semver\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/9c/1a/93/41fad0b4691117752df4acca090024f7e8b8175ac5cc77aae8\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/14/ea/04/c1596ef3d869372d0fc209d5db0fcdd551151cfc0d620570f5\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
      "  Running setup.py bdist_wheel for flexmock ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/fb/2e/55/55fef7b013e35ce2d2d6bffc213a3043e1a18730848e3953ee\n",
      "  Running setup.py bdist_wheel for semver ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/71/6e/e3/d8d40ba8bbebb9210e1fc85a346110fad103d1d82b1e4de604\n",
      "Successfully built spacy thinc pathlib ujson flexmock semver\n",
      "Installing collected packages: murmurhash, cymem, preshed, tqdm, plac, flexmock, thinc, pathlib, semver, sputnik, ujson, spacy\n",
      "Successfully installed cymem-1.31.2 flexmock-0.10.2 murmurhash-0.26.4 pathlib-1.0.1 plac-0.9.6 preshed-0.46.4 semver-2.7.4 spacy-1.6.0 sputnik-0.9.3 thinc-6.2.0 tqdm-4.11.2 ujson-1.35\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-0.13.4.1-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.8MB 110kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.7.0 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.3 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.3.5.tar.gz\n",
      "Requirement already satisfied: boto>=2.32 in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /Users/edmond_20000/anaconda/lib/python2.7/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/a6/61/e5/74ff1f24ad225557675acc0cc2ccb196d7b83c1eb7d80390c8\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/edmond_20000/Library/Caches/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, smart-open, gensim\n",
      "Successfully installed bz2file-0.98 gensim-0.13.4.1 smart-open-1.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edmond_20000/Desktop/Edmond-repo/lessons/lesson-14/code/solution-code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = codecs.open(filename, 'r', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"../../assets/dataset/captured-tweets.tx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'../../assets/dataset/captured-tweets.tx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-82d66d00bdd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/edmond_20000/anaconda/lib/python2.7/codecs.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__builtin__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'../../assets/dataset/captured-tweets.tx'"
     ]
    }
   ],
   "source": [
    "[ tweet for tweet in codecs.open(filename, 'r', encoding=\"utf-8\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the tweet data\n",
    "filename = '../../assets/dataset/captured-tweets.txt'\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "niip_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a700a47344d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirstTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfirstTweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "firstTweet = tweets(0)\n",
    "print firstTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nip_toolkit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3bf9cc981c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpareed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnip_toolkit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstTweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nip_toolkit' is not defined"
     ]
    }
   ],
   "source": [
    "pareed = nip_toolkit(firstTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a\n",
    "\n",
    "Write a function that can take a take a sentence parsed by `spacy` and identify if it mentions a company named 'Google'. Remember, `spacy` can find entities and codes them as `ORG` if they are a company. Look at the slides for class 13 if you need a hint:\n",
    "\n",
    "### Bonus (1b)\n",
    "\n",
    "Parameterize the company name so that the function works for any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mentions_company(parsed):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == \"Google\" and entity.label_ == 'ORG':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 1b\n",
    "\n",
    "def mentions_company(parsed, company='Google'):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == company and entity.label_ == 'ORG':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = nlp_toolkit(\"Bilind is a nice company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for entity in parsed.ents:\n",
    "    print entity.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1c\n",
    "\n",
    "Write a function that can take a sentence parsed by `spacy` \n",
    "and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "print spacy.parts_of_speech.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = []\n",
    "    for el in parsed:\n",
    "        if el.pos == spacy.parts_of_speech.VERB:\n",
    "            actions.append(el.text)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1d\n",
    "For each tweet, parse it using spacy and print it out if the tweet has 'release' or 'announce' as a verb. You'll need to use your `mentions_company` and `get_actions` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google &amp; Ford rumored to announce partnership at CES https://t.co/zOgm1NjHhD https://t.co/Gzx81ujqVC\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/6woe56G22Q\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/4hERVJ4zZK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if mentions_company(parsed, 'Google'):\n",
    "        actions = get_actions(parsed)\n",
    "        if 'release' in actions or 'announce' in actions:\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1e\n",
    "Write a function that identifies countries - HINT: the entity label for countries is GPE (or GeoPolitical Entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_country(parsed, country):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == country and entity.label_ == 'GPE':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1f\n",
    "\n",
    "Re-run (d) to find country tweets that discuss 'Iran' announcing or releasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/TRshnC6sVU\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/SlvcQtk3vE\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "Hhmmm. Iran claiming to have 'warned Nigeria' to release detained Shiite leader.... @afalli\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "\n",
    "    if mentions_country(parsed, 'Iran'):\n",
    "        actions = get_actions(parsed)\n",
    "        if 'release' in actions or 'announce' in actions:\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = map(lambda line: unicode(line), tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Build a word2vec model of the tweets we have collected using gensim.\n",
    "First take the collection of tweets and tokenize them using spacy.\n",
    "\n",
    "### Exercise 2a:\n",
    "* Think about how this should be done. \n",
    "* Should you only use upper-case or lower-case? \n",
    "* Should you remove punctuations or symbols? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                for x in nlp_toolkit(t)] for t in tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "Build a word2vec model.\n",
    "Test the window size as well - this is how many surrounding words need to be used to model a word. What do you think is appropriate for Twitter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "Test your word2vec model with a few similarity functions. \n",
    "* Find words similar to 'Syria'.\n",
    "* Find words similar to 'war'.\n",
    "* Find words similar to \"Iran\".\n",
    "* Find words similar to 'Verizon'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Syria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2d\n",
    "\n",
    "Adjust the choices in (b) and (c) as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Filter tweets to those that mention 'Iran' or similar entities and 'war' or similar entities.\n",
    "* Do this using just spacy.\n",
    "* Do this using word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using spacy\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if mentions_country(parsed, 'Iran') or mentions_country(parsed, 'Iraq'): # ... you could add more\n",
    "        if 'attack' in get_actions(parsed):\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using word2vec similarity scores\n",
    "for tweet in tweets[:200]:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "\n",
    "    similarity_to_iran = max([model.similarity('Iran', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "    similarity_to_war = max([model.similarity('war', tok.text) for tok in parsed if tok.text in model.vocab], 0)\n",
    "    if similarity_to_iran > 0.9 and similarity_to_war > 0.9:\n",
    "#         print(similarity_to_iran, similarity_to_war, tweet)\n",
    "        print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
